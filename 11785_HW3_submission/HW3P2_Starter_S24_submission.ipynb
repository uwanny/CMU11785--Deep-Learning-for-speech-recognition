{"cells":[{"cell_type":"markdown","metadata":{"id":"UR4qfYrVoO4v"},"source":["# Installs"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136824,"status":"ok","timestamp":1711403825850,"user":{"displayName":"Suwan Sun","userId":"12592220404673839974"},"user_tz":240},"id":"mA9qZoIDcx-h","outputId":"ac1c1bcb-43c7-424d-bc67-e5f5ef65b1f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6286,"status":"ok","timestamp":1711407024420,"user":{"displayName":"Suwan Sun","userId":"12592220404673839974"},"user_tz":240},"id":"KUUtbautm7Hq","outputId":"ef522bd6-1215-4012-d624-be367a00845e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  git-man liberror-perl\n","Suggested packages:\n","  git-daemon-run | git-daemon-sysvinit git-doc git-el git-email git-gui gitk\n","  gitweb git-cvs git-mediawiki git-svn\n","The following NEW packages will be installed:\n","  git git-man liberror-perl\n","0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n","Need to get 7285 kB of archives.\n","After this operation, 38.1 MB of additional disk space will be used.\n","Do you want to continue? [Y/n] ^C\n"]}],"source":["!sudo apt-get install git"]},{"cell_type":"markdown","metadata":{"id":"ONgAWhqdoYy-"},"source":["\n","This may take a while"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31392,"status":"ok","timestamp":1711407062878,"user":{"displayName":"Suwan Sun","userId":"12592220404673839974"},"user_tz":240},"id":"SS7a7xeEoaV9","outputId":"973f5e3c-8ea7-4488-9ba9-a466b56f91a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n","/home/22941/ctcdecode\n","/home/22941\n"]}],"source":["!pip install wandb --quiet\n","!pip install python-Levenshtein -q\n","!git clone --recursive https://github.com/parlance/ctcdecode.git\n","!pip install wget -q\n","%cd ctcdecode\n","!pip install . -q\n","%cd ..\n","\n","# !pip install torchsummaryX -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HKLIou7YPo2Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: torchsummaryx==1.3.0 in ./.local/lib/python3.7/site-packages (1.3.0)\n","Requirement already satisfied: torch in ./.local/lib/python3.7/site-packages (from torchsummaryx==1.3.0) (1.13.1+cu117)\n","Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchsummaryx==1.3.0) (1.21.6)\n","Requirement already satisfied: pandas in ./.local/lib/python3.7/site-packages (from torchsummaryx==1.3.0) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in ./.local/lib/python3.7/site-packages (from pandas->torchsummaryx==1.3.0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2017.3 in ./.local/lib/python3.7/site-packages (from pandas->torchsummaryx==1.3.0) (2024.1)\n","Requirement already satisfied: typing-extensions in ./.local/lib/python3.7/site-packages (from torch->torchsummaryx==1.3.0) (4.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryx==1.3.0) (1.12.0)\n"]}],"source":["'''\n","If torchsummaryX doesn't work, please run this cell. Alternatively, please refer to Piazza post @209 for more assistance:\n","'''\n","\n","!pip install torchsummaryx==1.3.0"]},{"cell_type":"markdown","metadata":{"id":"IWVONJxCobPc"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true},"id":"78ZTCIXoof2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummaryX import summary\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","import torchaudio.transforms as tat\n","\n","from sklearn.metrics import accuracy_score\n","import gc\n","\n","import zipfile\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import datetime\n","\n","# imports for decoding and distance calculation\n","import ctcdecode\n","import Levenshtein\n","from ctcdecode import CTCBeamDecoder\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"gg3-yJ8tok34"},"source":["# Kaggle Setup"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"AdUelfGhom1m"},"outputs":[{"name":"stdout","output_type":"stream","text":["chmod: cannot access '/root/.kaggle/kaggle.json': Permission denied\n"]}],"source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n","# !mkdir /root/.kaggle\n","\n","# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","#     f.write('{\"username\":\"\",\"key\":\"\"}') # TODO: Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dSjBwfXeoq4B","outputId":"0f4b1ed1-828d-4711-c978-65a3d2e94404"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (5.2.0) doesn't match a supported version!\n","  RequestsDependencyWarning)\n","Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/22941/.kaggle/kaggle.json'\n","Downloading hw3p2asr-s24.zip to /home/22941\n","100%|██████████████████████████████████████▉| 3.73G/3.74G [00:27<00:00, 157MB/s]\n","100%|███████████████████████████████████████| 3.74G/3.74G [00:27<00:00, 148MB/s]\n"]}],"source":["!kaggle competitions download -c hw3p2asr-s24"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_ruxWP60LCQA","outputId":"a8eb6282-4a47-4661-8acf-af78e800b947"},"outputs":[{"name":"stdout","output_type":"stream","text":["11-785-s24-hw3p2\t\t    ctcdecode\t      install_gpu_driver.py\n","HW3P2_Starter_S24_fromVM.ipynb\t    get-pip.py\t      setup.sh\n","NVIDIA-Linux-x86_64-525.125.06.run  hw3p2asr-s24.zip\n"]}],"source":["'''\n","This will take a couple minutes, but you should see at least the following:\n","11-785-s24-hw3p2  ctcdecode  hw3p2asr-s24.zip  sample_data\n","'''\n","!unzip -q hw3p2asr-s24.zip\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"R9v5ewZDMpYA"},"source":["# Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Cp-716IMZRd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"2ORNHnSFroP0"},"source":["# Dataset and Dataloader"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"k0v7wHRWrqH6"},"outputs":[],"source":["# ARPABET PHONEME MAPPING\n","# DO NOT CHANGE\n","\n","CMUdict_ARPAbet = {\n","    \"\" : \" \",\n","    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n","    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n","    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n","    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n","    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n","    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n","    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n","    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n","    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n","}\n","\n","CMUdict = list(CMUdict_ARPAbet.keys())\n","ARPAbet = list(CMUdict_ARPAbet.values())\n","\n","\n","PHONEMES = CMUdict[:-2]\n","LABELS = ARPAbet[:-2]"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eN2kcxwXLLBb","outputId":"a1cb3aaa-413b-4f56-a1e2-2714b6ff78c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['', '[SIL]', 'NG', 'F', 'M', 'AE', 'R', 'UW', 'N', 'IY', 'AW', 'V', 'UH', 'OW', 'AA', 'ER', 'HH', 'Z', 'K', 'CH', 'W', 'EY', 'ZH', 'T', 'EH', 'Y', 'AH', 'B', 'P', 'TH', 'DH', 'AO', 'G', 'L', 'JH', 'OY', 'SH', 'D', 'AY', 'S', 'IH', '[SOS]', '[EOS]']   [' ', '-', 'G', 'f', 'm', '@', 'r', 'u', 'n', 'i', 'W', 'v', 'U', 'o', 'a', 'R', 'h', 'z', 'k', 'C', 'w', 'e', 'Z', 't', 'E', 'y', 'A', 'b', 'p', 'T', 'D', 'c', 'g', 'l', 'j', 'O', 'S', 'd', 'Y', 's', 'I', '[SOS]', '[EOS]']\n"]}],"source":["# You might want to play around with the mapping as a sanity check here\n","print(CMUdict,' ',ARPAbet)"]},{"cell_type":"markdown","metadata":{"id":"agmNBKf4JrLV"},"source":["### Train Data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"afd0_vlbJmr_"},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","\n","    #TODO\n","    def __init__(self, root, PHONEMES=PHONEMES, partition= \"train-clean-100/\", limit = None, transforms = True):\n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        '''\n","        self.transforms = transforms\n","        # Load the directory and all files in them\n","\n","        self.mfcc_dir = os.path.join(root,partition,'mfcc')\n","        self.transcript_dir = os.path.join(root,partition,'transcript')\n","\n","        if limit is None:\n","            self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n","            self.transcript_files = sorted(os.listdir(self.transcript_dir))\n","        else:\n","            self.mfcc_files = sorted(os.listdir(self.mfcc_dir))[:limit]\n","            self.transcript_files = sorted(os.listdir(self.transcript_dir))[:limit]\n","\n","        self.PHONEMES = PHONEMES\n","\n","        self.mfccs, self.transcripts = [], []\n","\n","        # Iterate through mfccs and transcripts\n","        for i in range(len(self.mfcc_files)):\n","        #   Load a single mfcc\n","            mfcc        = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i])) # this is the actual values\n","        #   Do Cepstral Normalization of mfcc (explained in writeup for hw1p1)\n","            mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n","        #   Load the corresponding transcript\n","            transcript  = np.load(os.path.join(self.transcript_dir,self.transcript_files[i])) # Remove [SOS] and [EOS] from the transcript\n","            transcript = transcript[1:-1]\n","            # (Is there an efficient way to do this without traversing through the transcript?)\n","            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n","            self.mfccs.append(mfcc)\n","            #self.transcripts.append(transcript)\n","\n","            self.transcripts.append(np.array([self.PHONEMES.index(i) for i in transcript]))\n","\n","\n","        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n","        # Each transcript is of shape (T1_out+2), (T2_out+2) before removing [SOS] and [EOS]\n","\n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfccs)\n","\n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n","        # Map the phonemes to their corresponding list indexes in self.phonemes\n","        # phonemes_map = {phoneme: int(index) for index, phoneme in enumerate(self.PHONEMES)}\n","        # # labe_map = {label : index for index, label in enumerate()}\n","        # for i, transcript in enumerate(self.transcripts): # This will iterate the self.transcript list\n","        #     # for j in range(len(transcript)): # This will iterate the each transcript in self.transcript list\n","        #     self.transcripts[i] = phonemes_map[transcript]\n","        # #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","\n","\n","    def __len__(self):\n","\n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","\n","        Once done, return a tuple of features and labels.\n","        '''\n","\n","        mfcc =  self.mfccs[ind] #This will return the mfcc of shape[1,27]\n","        transcript =  self.transcripts[ind]#Now here we want the correspond transcript\n","        # mfcc      = torch.FloatTensor(mfcc) # Convert to tensors\n","        # transcript    = torch.tensor(transcript)\n","\n","        return mfcc, transcript\n","\n","\n","    def collate_fn(self,batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish.\n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features,\n","            and lengths of labels.\n","        '''\n","        batch_mfcc = [item[0] for item in batch]  # item[0]\n","        batch_transcript = [item[1] for item in batch]  # item[1]\n","\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","\n","        #This is to store the original length information\n","        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n","        lengths_transcript = [len(transcript) for transcript in batch_transcript]\n","\n","        batch_mfcc_pad = pad_sequence(sequences=[torch.tensor(mfcc) for mfcc in batch_mfcc],batch_first=True, padding_value=0)\n","        # print(batch_mfcc_pad.shape)\n","        batch_transcript_pad = pad_sequence(sequences=[torch.tensor(transcript) for transcript in batch_transcript], batch_first=True, padding_value=0)\n","        # print(batch_transcript_pad.shape)\n","        if self.transforms == True:\n","            time_mask = tat.TimeMasking(time_mask_param=80,iid_masks=True)\n","            frequency_mask = tat.FrequencyMasking(freq_mask_param=2,iid_masks=True)\n","            batch_mfcc_pad = torch.permute(batch_mfcc_pad, (0, 2, 1))\n","            batch_mfcc_pad = time_mask(batch_mfcc_pad)\n","            batch_mfcc_pad = frequency_mask(batch_mfcc_pad)\n","            batch_mfcc_pad = torch.permute(batch_mfcc_pad, (0, 2, 1))\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","\n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hqDrxeHfJw4g"},"source":["### Test Data"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HrLS1wfVJppA"},"outputs":[],"source":["# Test Dataloader\n","#TODO\n","class AudioDatasetTest(torch.utils.data.Dataset):\n","   #TODO\n","    def __init__(self, root, PHONEMES=PHONEMES, partition= None, limit = None):\n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        '''\n","\n","        # Load the directory and all files in them\n","\n","        self.mfcc_dir = os.path.join(root,partition,'mfcc')\n","\n","        if limit is None:\n","            self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n","        else:\n","            self.mfcc_files = sorted(os.listdir(self.mfcc_dir))[:limit]\n","\n","        self.PHONEMES = PHONEMES\n","\n","        self.mfccs = []\n","\n","        # Iterate through mfccs and transcripts\n","        for i in range(len(self.mfcc_files)):\n","        #   Load a single mfcc\n","            mfcc        = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i])) # this is the actual values\n","        #   Do Cepstral Normalization of mfcc (explained in writeup for hw1p1)\n","            mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n","            self.mfccs.append(mfcc)\n","\n","\n","        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n","        # Each transcript is of shape (T1_out+2), (T2_out+2) before removing [SOS] and [EOS]\n","\n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfccs)\n","\n","\n","    def __len__(self):\n","\n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","\n","        Once done, return a tuple of features and labels.\n","        '''\n","\n","        mfcc =  self.mfccs[ind] #This will return the mfcc of shape[1,27]\n","        return mfcc\n","\n","    def collate_fn(self,batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish.\n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features,\n","            and lengths of labels.\n","        '''\n","        batch_mfcc = [item for item in batch]  # item[0]\n","\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","\n","        #This is to store the original length information\n","        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n","\n","        batch_mfcc_pad = pad_sequence(sequences=[torch.tensor(mfcc) for mfcc in batch_mfcc],batch_first=True, padding_value=0)\n","        # print(batch_mfcc_pad.shape)\n","        # print(batch_transcript_pad.shape)\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","\n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"]},{"cell_type":"markdown","metadata":{"id":"Pt-veYcdL6Fe"},"source":["### Config - Hyperparameters"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MN82c3KpLup8"},"outputs":[],"source":["# root = '/content/11-785-s24-hw3p2/'\n","\n","# Feel free to add more items here\n","config = {\n","    \"beam_width\" : 3,\n","    \"lr\"         : 2e-3,\n","    \"epochs\"     : 50,\n","    \"batch_size\" : 64  # Increase if your device can handle it\n","}\n","\n","# You may pass this as a parameter to the dataset class above\n","# This will help modularize your implementation\n","transforms = [] # set of tranformations"]},{"cell_type":"markdown","metadata":{"id":"NmuPk9J6L8dz"},"source":["### Data loaders"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3_kG0gU2x4hH","outputId":"be7bc2a3-cd1b-4a89-e234-d759e5deeeec"},"outputs":[{"data":{"text/plain":["63"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# get me RAMMM!!!!\n","gc.collect()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4mzoYfTKu14s","outputId":"fc25bd82-68f3-4ec2-edf9-78aae4226687"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size:  64\n","Train dataset samples = 28539, batches = 446\n","Val dataset samples = 2703, batches = 43\n","Test dataset samples = 2620, batches = 41\n"]}],"source":["# Create objects for the dataset class\n","train_data = AudioDataset(root='11-785-s24-hw3p2/', PHONEMES=PHONEMES, partition='train-clean-100/', limit=None, transforms=True) #TODO\n","val_data = AudioDataset(root='11-785-s24-hw3p2', PHONEMES=PHONEMES, partition='dev-clean/', limit=None, transforms=False)# TODO : You can either use the same class with some modifications or make a new one :)\n","test_data = AudioDatasetTest(root='11-785-s24-hw3p2', PHONEMES=PHONEMES, partition='test-clean/', limit=None) #TODO\n","\n","# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_data,\n","    num_workers = 1,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = True,\n","    collate_fn  = train_data.collate_fn\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    dataset     = val_data,\n","    num_workers = 1,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = False,\n","    collate_fn  = val_data.collate_fn\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    dataset     = test_data,\n","    num_workers = 1,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = False,\n","    collate_fn  = test_data.collate_fn\n",")\n","\n","print(\"Batch size: \", config['batch_size'])\n","print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n","print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n","# x, y = train_data.collate_fn(5)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"cXMtwyviKaxK","outputId":"71ae2f67-7a9e-4235-b209-665421dd4205"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 1688, 27]) torch.Size([64, 195]) torch.Size([64]) torch.Size([64])\n","torch.Size([64, 2001, 27]) torch.Size([64])\n","torch.Size([64, 2363, 27]) torch.Size([64])\n","torch.Size([64, 3000, 27]) torch.Size([64])\n","torch.Size([64, 2052, 27]) torch.Size([64])\n","torch.Size([64, 2329, 27]) torch.Size([64])\n","torch.Size([64, 1818, 27]) torch.Size([64])\n","torch.Size([64, 1013, 27]) torch.Size([64])\n","torch.Size([64, 3387, 27]) torch.Size([64])\n","torch.Size([64, 3161, 27]) torch.Size([64])\n","torch.Size([64, 2590, 27]) torch.Size([64])\n","torch.Size([64, 1280, 27]) torch.Size([64])\n","torch.Size([64, 2114, 27]) torch.Size([64])\n","torch.Size([64, 1608, 27]) torch.Size([64])\n","torch.Size([64, 2714, 27]) torch.Size([64])\n","torch.Size([64, 2594, 27]) torch.Size([64])\n","torch.Size([64, 2823, 27]) torch.Size([64])\n","torch.Size([64, 2853, 27]) torch.Size([64])\n","torch.Size([64, 2607, 27]) torch.Size([64])\n","torch.Size([64, 960, 27]) torch.Size([64])\n","torch.Size([64, 3491, 27]) torch.Size([64])\n","torch.Size([64, 2956, 27]) torch.Size([64])\n","torch.Size([64, 2837, 27]) torch.Size([64])\n","torch.Size([64, 2910, 27]) torch.Size([64])\n","torch.Size([64, 974, 27]) torch.Size([64])\n","torch.Size([64, 2838, 27]) torch.Size([64])\n","torch.Size([64, 2147, 27]) torch.Size([64])\n","torch.Size([64, 1455, 27]) torch.Size([64])\n","torch.Size([64, 1509, 27]) torch.Size([64])\n","torch.Size([64, 3077, 27]) torch.Size([64])\n","torch.Size([64, 1327, 27]) torch.Size([64])\n","torch.Size([64, 2327, 27]) torch.Size([64])\n","torch.Size([64, 3284, 27]) torch.Size([64])\n","torch.Size([64, 2443, 27]) torch.Size([64])\n","torch.Size([64, 2280, 27]) torch.Size([64])\n","torch.Size([64, 2011, 27]) torch.Size([64])\n","torch.Size([64, 2837, 27]) torch.Size([64])\n","torch.Size([64, 2444, 27]) torch.Size([64])\n","torch.Size([64, 1632, 27]) torch.Size([64])\n","torch.Size([64, 2146, 27]) torch.Size([64])\n","torch.Size([64, 2078, 27]) torch.Size([64])\n","torch.Size([60, 3273, 27]) torch.Size([60])\n"]}],"source":["# sanity check\n","for data in train_loader:\n","    # m = data\n","    x, y, lx, ly = data\n","    print(x.shape, y.shape, lx.shape, ly.shape)\n","    break\n","\n","for data in test_loader:\n","    x,lx = data\n","    print(x.shape,lx.shape)"]},{"cell_type":"markdown","metadata":{"id":"wSexxhdfMUzx"},"source":["# NETWORK"]},{"cell_type":"markdown","metadata":{"id":"HLad4pChcuvX"},"source":["## Basic\n","\n","This is a basic block for understanding, you can skip this and move to pBLSTM one"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"EQhvHr71GJfq"},"outputs":[],"source":["# torch.cuda.empty_cache()\n","\n","# class Network(nn.Module):\n","\n","#     def __init__(self):\n","\n","#         super(Network, self).__init__()\n","\n","#         # Adding some sort of embedding layer or feature extractor might help performance.\n","#         # self.embedding = ?\n","\n","#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=256, num_layers=1, batch_first=True)\n","\n","#         self.classification = nn.Sequential(\n","#             nn.Linear(in_features=256, out_features=output_size)\n","#         )\n","\n","#         self.logSoftmax = nn.LogSoftmax(dim=1)\n","\n","\n","#         # TODO : look up the documentation. You might need to pass some additional parameters.\n","#         self.lstm = nn.LSTM(input_size = __, hidden_size = 256, num_layers = 1)\n","\n","#         self.classification = nn.Sequential(\n","#             #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n","#         )\n","\n","\n","#         self.logSoftmax = #TODO: Apply a log softmax here. Which dimension would apply it on ?\n","\n","#     def forward(self, x, lx):\n","#         #TODO\n","#         # The forward function takes 2 parameter inputs here. Why?\n","#         # Refer to the handout for hints\n","\n","#         # packing first to pass through the lstm\n","#         x_packed = nn.utils.rnn.pack_padded_sequence(x, lx, batch_first=True, enforce_sorted=False)\n","#         x_lstm, (h_n, c_n) = self.lstm(x_packed)\n","\n","#         # unpack the packed signal through classification layer\n","#         x_unpacked, _ = nn.utils.rnn.pad_packed_sequence(x_lstm, batch_first=True)\n","\n","#         # classify finally\n","#         x_classified = self.classification(x_unpacked)\n","\n","#         # final softmax\n","#         x_logsoftmax = self.logSoftmax(x_classified)\n","#         pass"]},{"cell_type":"markdown","metadata":{"id":"tUThsowyQdN7"},"source":["## Initialize Basic Network\n","(If trying out the basic Network)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CGoiXd70tb5z"},"outputs":[],"source":["# torch.cuda.empty_cache()\n","\n","# model = Network().to(device)\n","# summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"]},{"cell_type":"markdown","metadata":{"id":"e-qb7wnAzCZl"},"source":["## ASR Network"]},{"cell_type":"markdown","metadata":{"id":"PB6eh3gnMUzy"},"source":["### Pyramid Bi-LSTM (pBLSTM)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"qd4BEX_yMUzz"},"outputs":[],"source":["# Utils for network\n","torch.cuda.empty_cache()\n","# from torch.autograd import Variable\n","\n","class PermuteBlock(torch.nn.Module):\n","    def forward(self, x):\n","        return x.transpose(1, 2)\n","    \n","class LockedDropout(nn.Module):\n","    \"\"\" LockedDropout applies the same dropout mask to every time step.\n","\n","    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n","    their `License\n","    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n","\n","    Args:\n","        p (float): Probability of an element in the dropout mask to be zeroed.\n","    \"\"\"\n","\n","    def __init__(self, p=0.25):\n","        self.p = p\n","        super().__init__()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n","                apply dropout too.\n","        \"\"\"\n","        if not self.training or not self.p:\n","            return x\n","        x = x.clone()\n","        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n","        mask = mask.div_(1 - self.p)\n","        mask = mask.expand_as(x)\n","        return x * mask\n","\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' \\\n","            + 'p=' + str(self.p) + ')'\n","            \n","            \n","class Residual_Block(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","\n","        self.conv1 = torch.nn.Conv1d(in_channels, 64, kernel_size=3, padding=1, stride=1)\n","        self.bn1 = torch.nn.BatchNorm1d(64)\n","        self.conv2 = torch.nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=1)\n","        self.bn2 = torch.nn.BatchNorm1d(128)\n","        self.conv3 = torch.nn.Conv1d(128, out_channels, kernel_size=3, padding=1, stride=1)\n","        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n","        self.relu = torch.nn.ReLU()\n","        self.shortcut = torch.nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = torch.nn.Sequential(\n","                torch.nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","                torch.nn.BatchNorm1d(out_channels)\n","            )\n","    def forward(self, x):\n","        identity = self.shortcut(x)\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        \n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        \n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OmdyXI6KMUzz"},"outputs":[],"source":["class pBLSTM(torch.nn.Module):\n","\n","    '''\n","    Pyramidal BiLSTM\n","    Read the write up/paper and understand the concepts and then write your implementation here.\n","\n","    At each step,\n","    1. Pad your input if it is packed (Unpack it)\n","    2. Reduce the input length dimension by concatenating feature dimension\n","        (Tip: Write down the shapes and understand)\n","        (i) How should  you deal with odd/even length input?\n","        (ii) How should you deal with input length array (x_lens) after truncating the input?\n","    3. Pack your input\n","    4. Pass it into LSTM layer\n","\n","    To make our implementation modular, we pass 1 layer at a time.\n","    '''\n","\n","    def __init__(self, input_size, hidden_size):\n","        super(pBLSTM, self).__init__()\n","\n","        self.blstm = torch.nn.LSTM(input_size=2*input_size, hidden_size=hidden_size, num_layers=3, bidirectional=True, dropout = 0.3)  # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n","        #could drop out\n","    def forward(self, x_packed): # x_packed is a PackedSequence\n","\n","        # TODO: Pad Packed Sequence\n","        seq_unpacked, lens_unpacked = pad_packed_sequence(x_packed, batch_first=True)\n","        # print(f'in the plstm section after pad {seq_unpacked.shape},{lens_unpacked.shape}')\n","        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n","        x, x_lens = self.trunc_reshape(x=seq_unpacked, x_lens=lens_unpacked)\n","        # print(f'in the plstm section after truncate {x.shape},{x_lens.shape}')\n","        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n","        # TODO: Pack Padded Sequence. What output(s) would you get?\n","        PackedSequence = pack_padded_sequence(input=x,lengths=x_lens,batch_first=True,enforce_sorted=False)\n","        # TODO: Pass the sequence through bLSTM\n","        packed_output, (_, _) = self.blstm(PackedSequence)\n","        # What do you return?\n","\n","        return packed_output\n","\n","    def trunc_reshape(self, x, x_lens):\n","        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n","        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n","        # TODO: Reduce lengths by the same downsampling factor\n","        # The shape of x is [batch_size, frames, features]\n","        batch_size = x.shape[0]\n","        frames = x.shape[1]\n","        features = x.shape[2]\n","        if frames % 2 != 0:\n","            x = x[:, :-1, :]\n","        x = x.reshape((batch_size, x.shape[1]//2, features*2))\n","        x_lens = x_lens/2\n","        return x, x_lens"]},{"cell_type":"markdown","metadata":{"id":"g3ZQ75OcMUz0"},"source":["### Encoder"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"GEzw5_xmMUz0"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    '''\n","    The Encoder takes utterances as inputs and returns latent feature representations\n","    '''\n","    def __init__(self, input_size, encoder_hidden_size):\n","        super(Encoder, self).__init__()\n","\n","        self.PermuteBlock = PermuteBlock()\n","        self.embedding = torch.nn.Sequential(torch.nn.Conv1d(in_channels=input_size,out_channels=64,kernel_size=3,padding=1,stride=1),\n","                                             torch.nn.BatchNorm1d(64),\n","                                             torch.nn.GELU(),\n","                                             torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n","                                             torch.nn.BatchNorm1d(128),\n","                                             torch.nn.GELU(),\n","                                             torch.nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n","                                             torch.nn.BatchNorm1d(256),\n","                                             torch.nn.GELU()\n","                                             )\n","        # self.embedding = torch.nn.Sequential(Residual_Block(in_channels=input_size, out_channels=encoder_hidden_size,stride=1)\n","        #                                     #  Residual_Block(in_channels=encoder_hidden_size, out_channels=encoder_hidden_size, stride=1),\n","        #                                     #  Residual_Block(in_channels=encoder_hidden_size, out_channels=encoder_hidden_size, stride=1)\n","        #                                      )\n","        # self.embedding = torch.nn.Conv1d(in_channels=input_size,out_channels=encoder_hidden_size,kernel_size=3,padding=1,stride=1)\n","        # self.conv1 = torch.nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1)#TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n","        # self.batchnorm1 = torch.nn.BatchNorm1d(64)\n","        # self.relu = torch.nn.ReLU()\n","        # self.conv2 = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)#TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n","        # self.batchnorm2 = torch.nn.BatchNorm1d(encoder_hidden_size)\n","        # self.conv3 = torch.nn.Conv1d(in_channels=64, out_channels=encoder_hidden_size, kernel_size=3, stride=1, padding=1)\n","        # self.batch\n","        # self.shortcut = torch.nn.Sequential(torch.nn.Conv1d(in_channels=input_size, out_channels=encoder_hidden_size, kernel_size=3, stride=1, padding=1), torch.nn.BatchNorm1d(encoder_hidden_size))\n","        # could dropout, batchnorm, RELU, and residual connection\n","        \n","        # self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n","        #     # TODO: Fill this up with pBLSTMs - What should the input_size be?\n","        #     # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n","        #     # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n","        #     # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n","        #     # ...\n","        #     # ...\n","        #     pBLSTM(input_size=encoder_hidden_size,hidden_size=encoder_hidden_size),\n","        #     # LockedDropout(),\n","        #     pBLSTM(input_size=2*encoder_hidden_size,hidden_size=encoder_hidden_size),\n","        #     # LockedDropout(),\n","        #     pBLSTM(input_size=2*encoder_hidden_size,hidden_size=encoder_hidden_size)\n","        #     # LockedDropout()\n","        # )\n","        # self.pBLSTM1 = pBLSTM(input_size=encoder_hidden_size,hidden_size=encoder_hidden_size)\n","        # self.dropout1 = LockedDropout()\n","        # self.pBLSTM2 = pBLSTM(input_size=2*encoder_hidden_size,hidden_size=encoder_hidden_size)\n","        # self.pBLSTM3 = pBLSTM(input_size=2*encoder_hidden_size,hidden_size=encoder_hidden_size)\n","        self.lstm_layers = torch.nn.ModuleList()\n","        for i in range(2):\n","            input_size_pblstm = 256 if i == 0 else encoder_hidden_size * 2\n","            self.lstm_layers.append(pBLSTM(input_size=input_size_pblstm, hidden_size=encoder_hidden_size))\n","            self.lstm_layers.append(LockedDropout())\n","        \n","    def forward(self, x, x_lens):\n","        # Where are x and x_lens coming from? The dataloader\n","        #TODO: Call the permute layer and embedding layer\n","        x = self.PermuteBlock(x)\n","        x = self.embedding(x)\n","        x = self.PermuteBlock(x)\n","        # # TODO: Pack Padded Sequence\n","        # packed = pack_padded_sequence(input=x,lengths=x_lens,batch_first=True,enforce_sorted=False)\n","        # # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n","        # x = self.pBLSTM1(packed)\n","        # # TODO: Pad Packed Sequence\n","        # encoder_outputs, encoder_lens= pad_packed_sequence(sequence=x,batch_first=True)\n","        # print(f'The output from plstm, {encoder_outputs.shape},{encoder_lens.shape}')\n","        for layer in self.lstm_layers:\n","            if isinstance(layer, pBLSTM):\n","                x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n","                x_packed = layer(x_packed)\n","                x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","            else:\n","                x = torch.permute(x, (1, 0, 2))\n","                x = layer(x)\n","                x = torch.permute(x,(1,0,2))\n","        # Remember the number of output(s) each function returns\n","\n","        return x, x_lens\n","    "]},{"cell_type":"markdown","metadata":{"id":"kg82HXa3MUz1"},"source":["### Decoder"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"PQIRxdNTMUz1"},"outputs":[],"source":["class Decoder(torch.nn.Module):\n","\n","    def __init__(self, embed_size, output_size= 41):\n","        super().__init__()\n","\n","        self.mlp = torch.nn.Sequential( #maybe change the hidden size to be normal 256,512...\n","            PermuteBlock(), torch.nn.BatchNorm1d(embed_size), PermuteBlock(),\n","            #TODO define your MLP arch. Refer HW1P2\n","            #Use Permute Block before and after BatchNorm1d() to match the size\n","            torch.nn.Linear(in_features=embed_size,out_features=128),\n","            torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            torch.nn.Linear(in_features=128, out_features=64),\n","            torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            torch.nn.Linear(in_features=64, out_features=output_size)\n","            # PermuteBlock(), torch.nn.BatchNorm1d(512*4), PermuteBlock(),\n","            # torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            # torch.nn.Linear(in_features=512*4, out_features=512*8),\n","            # PermuteBlock(), torch.nn.BatchNorm1d(512*8), PermuteBlock(),\n","            # torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            # torch.nn.Linear(in_features=512*8, out_features=512*4),\n","            # PermuteBlock(), torch.nn.BatchNorm1d(512*4), PermuteBlock(),\n","            # torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            # torch.nn.Linear(in_features=512*4, out_features=512*2),\n","            # PermuteBlock(), torch.nn.BatchNorm1d(512*2), PermuteBlock(),\n","            # torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            # torch.nn.Linear(in_features=512*2, out_features=512*1),\n","            # PermuteBlock(), torch.nn.BatchNorm1d(512*1), PermuteBlock(),\n","            # torch.nn.GELU(),\n","            # torch.nn.Dropout(p=0.2),\n","            # torch.nn.Linear(in_features=512*1, out_features=output_size)\n","        )\n","\n","        self.softmax = torch.nn.LogSoftmax(dim=2)\n","\n","    def forward(self, encoder_out):\n","        #TODO call your MLP\n","        out = self.mlp(encoder_out)\n","        #TODO Think what should be the final output of the decoder for the classification\n","        decoder_out = self.softmax(out)\n","        return decoder_out"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"qmHf6pFiMUz1"},"outputs":[],"source":["class ASRModel(torch.nn.Module):\n","\n","    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n","        super().__init__()\n","\n","        self.augmentations  = torch.nn.Sequential(\n","            #TODO Add Time Masking/ Frequency Masking\n","            #Hint: See how to use PermuteBlock() function defined above\n","        )\n","        self.encoder        = Encoder(input_size=input_size, encoder_hidden_size=embed_size*2)\n","        self.decoder        = Decoder(embed_size=4* embed_size, output_size=41) # this is because the encoder double the hidden size\n","\n","\n","\n","    def forward(self, x, lengths_x):\n","\n","        if self.training:\n","            x = self.augmentations(x)\n","\n","        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n","        decoder_out                 = self.decoder(encoder_out)\n","        # print(f'The final dimension: {decoder_out.shape},{encoder_lens.shape}')\n","        return decoder_out, encoder_lens"]},{"cell_type":"markdown","metadata":{"id":"EV7DMPDoMUz2"},"source":["## Initialize ASR Network"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"oaaDsnnLMUz2","outputId":"174ec9a1-3818-4328-f911-86802cbb155b"},"outputs":[{"name":"stdout","output_type":"stream","text":["ASRModel(\n","  (augmentations): Sequential()\n","  (encoder): Encoder(\n","    (PermuteBlock): PermuteBlock()\n","    (embedding): Sequential(\n","      (0): Conv1d(27, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): GELU(approximate='none')\n","      (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): GELU(approximate='none')\n","      (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): GELU(approximate='none')\n","    )\n","    (lstm_layers): ModuleList(\n","      (0): pBLSTM(\n","        (blstm): LSTM(512, 384, num_layers=3, dropout=0.3, bidirectional=True)\n","      )\n","      (1): LockedDropout(p=0.25)\n","      (2): pBLSTM(\n","        (blstm): LSTM(1536, 384, num_layers=3, dropout=0.3, bidirectional=True)\n","      )\n","      (3): LockedDropout(p=0.25)\n","    )\n","  )\n","  (decoder): Decoder(\n","    (mlp): Sequential(\n","      (0): PermuteBlock()\n","      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): PermuteBlock()\n","      (3): Linear(in_features=768, out_features=128, bias=True)\n","      (4): GELU(approximate='none')\n","      (5): Linear(in_features=128, out_features=64, bias=True)\n","      (6): GELU(approximate='none')\n","      (7): Linear(in_features=64, out_features=41, bias=True)\n","    )\n","    (softmax): LogSoftmax(dim=2)\n","  )\n",")\n","===============================================================================================\n","                                         Kernel Shape     Output Shape  \\\n","Layer                                                                    \n","0_augmentations                                     -   [64, 2936, 27]   \n","1_encoder.PermuteBlock_PermuteBlock                 -   [64, 27, 2936]   \n","2_encoder.embedding.Conv1d_0              [27, 64, 3]   [64, 64, 2936]   \n","3_encoder.embedding.BatchNorm1d_1                [64]   [64, 64, 2936]   \n","4_encoder.embedding.GELU_2                          -   [64, 64, 2936]   \n","5_encoder.embedding.Conv1d_3             [64, 128, 3]  [64, 128, 2936]   \n","6_encoder.embedding.BatchNorm1d_4               [128]  [64, 128, 2936]   \n","7_encoder.embedding.GELU_5                          -  [64, 128, 2936]   \n","8_encoder.embedding.Conv1d_6            [128, 256, 3]  [64, 256, 2936]   \n","9_encoder.embedding.BatchNorm1d_7               [256]  [64, 256, 2936]   \n","10_encoder.embedding.GELU_8                         -  [64, 256, 2936]   \n","11_encoder.PermuteBlock_PermuteBlock                -  [64, 2936, 256]   \n","12_encoder.lstm_layers.0.LSTM_blstm                 -     [21060, 768]   \n","13_encoder.lstm_layers.LockedDropout_1              -  [1468, 64, 768]   \n","14_encoder.lstm_layers.2.LSTM_blstm                 -     [10517, 768]   \n","15_encoder.lstm_layers.LockedDropout_3              -   [734, 64, 768]   \n","16_decoder.mlp.PermuteBlock_0                       -   [64, 768, 734]   \n","17_decoder.mlp.BatchNorm1d_1                    [768]   [64, 768, 734]   \n","18_decoder.mlp.PermuteBlock_2                       -   [64, 734, 768]   \n","19_decoder.mlp.Linear_3                    [768, 128]   [64, 734, 128]   \n","20_decoder.mlp.GELU_4                               -   [64, 734, 128]   \n","21_decoder.mlp.Linear_5                     [128, 64]    [64, 734, 64]   \n","22_decoder.mlp.GELU_6                               -    [64, 734, 64]   \n","23_decoder.mlp.Linear_7                      [64, 41]    [64, 734, 41]   \n","24_decoder.LogSoftmax_softmax                       -    [64, 734, 41]   \n","\n","                                           Params    Mult-Adds  \n","Layer                                                           \n","0_augmentations                                 -            -  \n","1_encoder.PermuteBlock_PermuteBlock             -            -  \n","2_encoder.embedding.Conv1d_0               5.248k   15.220224M  \n","3_encoder.embedding.BatchNorm1d_1           128.0         64.0  \n","4_encoder.embedding.GELU_2                      -            -  \n","5_encoder.embedding.Conv1d_3              24.704k   72.155136M  \n","6_encoder.embedding.BatchNorm1d_4           256.0        128.0  \n","7_encoder.embedding.GELU_5                      -            -  \n","8_encoder.embedding.Conv1d_6               98.56k  288.620544M  \n","9_encoder.embedding.BatchNorm1d_7           512.0        256.0  \n","10_encoder.embedding.GELU_8                     -            -  \n","11_encoder.PermuteBlock_PermuteBlock            -            -  \n","12_encoder.lstm_layers.0.LSTM_blstm     9.848832M      9.8304M  \n","13_encoder.lstm_layers.LockedDropout_1          -            -  \n","14_encoder.lstm_layers.2.LSTM_blstm     12.99456M   12.976128M  \n","15_encoder.lstm_layers.LockedDropout_3          -            -  \n","16_decoder.mlp.PermuteBlock_0                   -            -  \n","17_decoder.mlp.BatchNorm1d_1               1.536k        768.0  \n","18_decoder.mlp.PermuteBlock_2                   -            -  \n","19_decoder.mlp.Linear_3                   98.432k      98.304k  \n","20_decoder.mlp.GELU_4                           -            -  \n","21_decoder.mlp.Linear_5                    8.256k       8.192k  \n","22_decoder.mlp.GELU_6                           -            -  \n","23_decoder.mlp.Linear_7                    2.665k       2.624k  \n","24_decoder.LogSoftmax_softmax                   -            -  \n","-----------------------------------------------------------------------------------------------\n","                           Totals\n","Total params           23.083689M\n","Trainable params       23.083689M\n","Non-trainable params          0.0\n","Mult-Adds             398.912768M\n","===============================================================================================\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_augmentations</th>\n","      <td>-</td>\n","      <td>[64, 2936, 27]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1_encoder.PermuteBlock_PermuteBlock</th>\n","      <td>-</td>\n","      <td>[64, 27, 2936]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_encoder.embedding.Conv1d_0</th>\n","      <td>[27, 64, 3]</td>\n","      <td>[64, 64, 2936]</td>\n","      <td>5248.0</td>\n","      <td>15220224.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_encoder.embedding.BatchNorm1d_1</th>\n","      <td>[64]</td>\n","      <td>[64, 64, 2936]</td>\n","      <td>128.0</td>\n","      <td>64.0</td>\n","    </tr>\n","    <tr>\n","      <th>4_encoder.embedding.GELU_2</th>\n","      <td>-</td>\n","      <td>[64, 64, 2936]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5_encoder.embedding.Conv1d_3</th>\n","      <td>[64, 128, 3]</td>\n","      <td>[64, 128, 2936]</td>\n","      <td>24704.0</td>\n","      <td>72155136.0</td>\n","    </tr>\n","    <tr>\n","      <th>6_encoder.embedding.BatchNorm1d_4</th>\n","      <td>[128]</td>\n","      <td>[64, 128, 2936]</td>\n","      <td>256.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>7_encoder.embedding.GELU_5</th>\n","      <td>-</td>\n","      <td>[64, 128, 2936]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8_encoder.embedding.Conv1d_6</th>\n","      <td>[128, 256, 3]</td>\n","      <td>[64, 256, 2936]</td>\n","      <td>98560.0</td>\n","      <td>288620544.0</td>\n","    </tr>\n","    <tr>\n","      <th>9_encoder.embedding.BatchNorm1d_7</th>\n","      <td>[256]</td>\n","      <td>[64, 256, 2936]</td>\n","      <td>512.0</td>\n","      <td>256.0</td>\n","    </tr>\n","    <tr>\n","      <th>10_encoder.embedding.GELU_8</th>\n","      <td>-</td>\n","      <td>[64, 256, 2936]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11_encoder.PermuteBlock_PermuteBlock</th>\n","      <td>-</td>\n","      <td>[64, 2936, 256]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12_encoder.lstm_layers.0.LSTM_blstm</th>\n","      <td>-</td>\n","      <td>[21060, 768]</td>\n","      <td>9848832.0</td>\n","      <td>9830400.0</td>\n","    </tr>\n","    <tr>\n","      <th>13_encoder.lstm_layers.LockedDropout_1</th>\n","      <td>-</td>\n","      <td>[1468, 64, 768]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14_encoder.lstm_layers.2.LSTM_blstm</th>\n","      <td>-</td>\n","      <td>[10517, 768]</td>\n","      <td>12994560.0</td>\n","      <td>12976128.0</td>\n","    </tr>\n","    <tr>\n","      <th>15_encoder.lstm_layers.LockedDropout_3</th>\n","      <td>-</td>\n","      <td>[734, 64, 768]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16_decoder.mlp.PermuteBlock_0</th>\n","      <td>-</td>\n","      <td>[64, 768, 734]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17_decoder.mlp.BatchNorm1d_1</th>\n","      <td>[768]</td>\n","      <td>[64, 768, 734]</td>\n","      <td>1536.0</td>\n","      <td>768.0</td>\n","    </tr>\n","    <tr>\n","      <th>18_decoder.mlp.PermuteBlock_2</th>\n","      <td>-</td>\n","      <td>[64, 734, 768]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19_decoder.mlp.Linear_3</th>\n","      <td>[768, 128]</td>\n","      <td>[64, 734, 128]</td>\n","      <td>98432.0</td>\n","      <td>98304.0</td>\n","    </tr>\n","    <tr>\n","      <th>20_decoder.mlp.GELU_4</th>\n","      <td>-</td>\n","      <td>[64, 734, 128]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>21_decoder.mlp.Linear_5</th>\n","      <td>[128, 64]</td>\n","      <td>[64, 734, 64]</td>\n","      <td>8256.0</td>\n","      <td>8192.0</td>\n","    </tr>\n","    <tr>\n","      <th>22_decoder.mlp.GELU_6</th>\n","      <td>-</td>\n","      <td>[64, 734, 64]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>23_decoder.mlp.Linear_7</th>\n","      <td>[64, 41]</td>\n","      <td>[64, 734, 41]</td>\n","      <td>2665.0</td>\n","      <td>2624.0</td>\n","    </tr>\n","    <tr>\n","      <th>24_decoder.LogSoftmax_softmax</th>\n","      <td>-</td>\n","      <td>[64, 734, 41]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         Kernel Shape     Output Shape  \\\n","Layer                                                                    \n","0_augmentations                                     -   [64, 2936, 27]   \n","1_encoder.PermuteBlock_PermuteBlock                 -   [64, 27, 2936]   \n","2_encoder.embedding.Conv1d_0              [27, 64, 3]   [64, 64, 2936]   \n","3_encoder.embedding.BatchNorm1d_1                [64]   [64, 64, 2936]   \n","4_encoder.embedding.GELU_2                          -   [64, 64, 2936]   \n","5_encoder.embedding.Conv1d_3             [64, 128, 3]  [64, 128, 2936]   \n","6_encoder.embedding.BatchNorm1d_4               [128]  [64, 128, 2936]   \n","7_encoder.embedding.GELU_5                          -  [64, 128, 2936]   \n","8_encoder.embedding.Conv1d_6            [128, 256, 3]  [64, 256, 2936]   \n","9_encoder.embedding.BatchNorm1d_7               [256]  [64, 256, 2936]   \n","10_encoder.embedding.GELU_8                         -  [64, 256, 2936]   \n","11_encoder.PermuteBlock_PermuteBlock                -  [64, 2936, 256]   \n","12_encoder.lstm_layers.0.LSTM_blstm                 -     [21060, 768]   \n","13_encoder.lstm_layers.LockedDropout_1              -  [1468, 64, 768]   \n","14_encoder.lstm_layers.2.LSTM_blstm                 -     [10517, 768]   \n","15_encoder.lstm_layers.LockedDropout_3              -   [734, 64, 768]   \n","16_decoder.mlp.PermuteBlock_0                       -   [64, 768, 734]   \n","17_decoder.mlp.BatchNorm1d_1                    [768]   [64, 768, 734]   \n","18_decoder.mlp.PermuteBlock_2                       -   [64, 734, 768]   \n","19_decoder.mlp.Linear_3                    [768, 128]   [64, 734, 128]   \n","20_decoder.mlp.GELU_4                               -   [64, 734, 128]   \n","21_decoder.mlp.Linear_5                     [128, 64]    [64, 734, 64]   \n","22_decoder.mlp.GELU_6                               -    [64, 734, 64]   \n","23_decoder.mlp.Linear_7                      [64, 41]    [64, 734, 41]   \n","24_decoder.LogSoftmax_softmax                       -    [64, 734, 41]   \n","\n","                                            Params    Mult-Adds  \n","Layer                                                            \n","0_augmentations                                NaN          NaN  \n","1_encoder.PermuteBlock_PermuteBlock            NaN          NaN  \n","2_encoder.embedding.Conv1d_0                5248.0   15220224.0  \n","3_encoder.embedding.BatchNorm1d_1            128.0         64.0  \n","4_encoder.embedding.GELU_2                     NaN          NaN  \n","5_encoder.embedding.Conv1d_3               24704.0   72155136.0  \n","6_encoder.embedding.BatchNorm1d_4            256.0        128.0  \n","7_encoder.embedding.GELU_5                     NaN          NaN  \n","8_encoder.embedding.Conv1d_6               98560.0  288620544.0  \n","9_encoder.embedding.BatchNorm1d_7            512.0        256.0  \n","10_encoder.embedding.GELU_8                    NaN          NaN  \n","11_encoder.PermuteBlock_PermuteBlock           NaN          NaN  \n","12_encoder.lstm_layers.0.LSTM_blstm      9848832.0    9830400.0  \n","13_encoder.lstm_layers.LockedDropout_1         NaN          NaN  \n","14_encoder.lstm_layers.2.LSTM_blstm     12994560.0   12976128.0  \n","15_encoder.lstm_layers.LockedDropout_3         NaN          NaN  \n","16_decoder.mlp.PermuteBlock_0                  NaN          NaN  \n","17_decoder.mlp.BatchNorm1d_1                1536.0        768.0  \n","18_decoder.mlp.PermuteBlock_2                  NaN          NaN  \n","19_decoder.mlp.Linear_3                    98432.0      98304.0  \n","20_decoder.mlp.GELU_4                          NaN          NaN  \n","21_decoder.mlp.Linear_5                     8256.0       8192.0  \n","22_decoder.mlp.GELU_6                          NaN          NaN  \n","23_decoder.mlp.Linear_7                     2665.0       2624.0  \n","24_decoder.LogSoftmax_softmax                  NaN          NaN  "]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model = ASRModel(\n","    input_size  = 27,\n","    embed_size  = 192,\n","    output_size = len(PHONEMES)\n",").to(device)\n","print(model)\n","summary(model, x.to(device), lx)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"UXQdpAJRm7Hz","outputId":"d632c99c-a188-48d3-9c40-8af6a85cd339"},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]}],"source":["import multiprocessing\n","n_cpus = multiprocessing.cpu_count()\n","print(n_cpus)"]},{"cell_type":"markdown","metadata":{"id":"IBwunYpyugFg"},"source":["# Training Config\n","Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"iGoozH2nd6KB"},"outputs":[],"source":["#TODO\n","criterion = torch.nn.CTCLoss(blank=0, reduction='mean',zero_infinity=True)  # Define CTC loss as the criterion. How would the losses be reduced?\n","# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n","# Refer to the handout for hints\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr= config['lr'], weight_decay=1e-2) #Defining Optimizer\n","# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n","# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n","decoder = CTCBeamDecoder(\n","    labels = LABELS, #\n","    model_path=None,\n","    alpha=0,\n","    beta=0,\n","    cutoff_top_n=40,\n","    cutoff_prob=1.0,\n","    beam_width=config['beam_width'],\n","    num_processes=1,\n","    blank_id=0,\n","    log_probs_input=True\n",")\n","\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, threshold=0.5, threshold_mode = 'rel',factor=0.8)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, threshold=1e-2, threshold_mode = 'abs',factor=0.8)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, threshold=0.1, threshold_mode = 'abs',factor=0.8, min_lr=5e-5)\n","# Mixed Precision, if you need it\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"Jmc6_4eWL2Xp"},"source":["# Decode Prediction"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"KHjnCDddL36E"},"outputs":[],"source":["def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n","\n","    # output = torch.permute(output, (1, 0, 2))\n","    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n","    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output, seq_lens = output_lens) #lengths - list of lengths\n","    pred_strings                    = []\n","\n","    for i in range(output_lens.shape[0]):\n","        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n","        # get the single batch result\n","        pred = beam_results[i][0][:out_lens[i][0]]  # we only care about the highest result\n","        #\n","        decoded_string = ''.join([PHONEME_MAP[p] for p in pred])\n","        pred_strings.append(decoded_string)\n","    return pred_strings # this returns for each batch\n","\n","def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n","\n","    dist            = 0\n","    batch_size      = label.shape[0]\n","\n","    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n","\n","    for i in range(batch_size):\n","        # TODO: Get predicted string and label string for each element in the batch\n","        pred_string = pred_strings[i]\n","        label_index = label[i, :label_lens[i]].tolist()\n","        label_string = ''.join([PHONEME_MAP[index] for index in label_index])\n","        dist += Levenshtein.distance(pred_string, label_string)\n","\n","    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n","\n","    return dist"]},{"cell_type":"markdown","metadata":{"id":"0Qk9iZud1LXT"},"source":["# Test Implementation"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"GnTLL-5gMBrY","outputId":"8709acbb-ec98-45bb-82b5-682dc6569e09"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 734, 41])\n","torch.Size([734, 64, 41]) torch.Size([64, 265])\n","tensor(7.6026, device='cuda:0', grad_fn=<MeanBackward0>)\n","203.703125\n"]}],"source":["# test code to check shapes\n","torch.cuda.empty_cache()\n","model.eval()\n","for i, data in enumerate(val_loader,0):\n","    x, y, lx, ly = data\n","    x, y = x.to(device), y.to(device)\n","    h, lh = model(x, lx)\n","    print(h.shape)\n","    h = torch.permute(h, (1, 0, 2))\n","    print(h.shape, y.shape)\n","    loss = criterion(h, y, lh, ly)\n","    print(loss)\n","    h = torch.permute(h, (1, 0, 2))\n","    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n","\n","    break"]},{"cell_type":"markdown","metadata":{"id":"rd5aNaLVoR_g"},"source":["# WandB\n","\n","You will need to fetch your api key from wandb.ai"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"PiDduMaDIARE"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/plain":["True"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","# from typing_extensions import Literal\n","wandb.login(key=\"b9fec08c02d03c41bdf5fe1f5268589277270fbe\")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"4s52yBOvICPZ"},"outputs":[{"data":{"text/html":["Finishing last run (ID:cj5rf9ps) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"018ba91c50cf47deb1971139a48552f3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='528.512 MB of 528.512 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▁▁</td></tr><tr><td>valid_dist</td><td>▅▁▇█</td></tr><tr><td>valid_loss</td><td>█▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.002</td></tr><tr><td>train_loss</td><td>3.34129</td></tr><tr><td>valid_dist</td><td>71.73961</td></tr><tr><td>valid_loss</td><td>3.29424</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">try17_TA</strong> at: <a href='https://wandb.ai/11785-hw3p2/hw3p2-ablations/runs/cj5rf9ps/workspace' target=\"_blank\">https://wandb.ai/11785-hw3p2/hw3p2-ablations/runs/cj5rf9ps/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240330_193221-cj5rf9ps/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:cj5rf9ps). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7dafd9c2aa04b4b909459777bef8d3e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114110611106297, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/22941/wandb/run-20240330_200220-b7ajwean</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/11785-hw3p2/hw3p2-ablations/runs/b7ajwean/workspace' target=\"_blank\">try17_TA</a></strong> to <a href='https://wandb.ai/11785-hw3p2/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/11785-hw3p2/hw3p2-ablations' target=\"_blank\">https://wandb.ai/11785-hw3p2/hw3p2-ablations</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/11785-hw3p2/hw3p2-ablations/runs/b7ajwean/workspace' target=\"_blank\">https://wandb.ai/11785-hw3p2/hw3p2-ablations/runs/b7ajwean/workspace</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["run = wandb.init(\n","    name = \"try17_TA\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # id = 'yvuki600',### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"6fLLj5KIMMOe"},"source":["# Train Functions"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"ri87MAdhMUz5"},"outputs":[],"source":["# from tqdm import tqdm\n","\n","def train_model(model, train_loader, criterion, optimizer):\n","\n","    model.train()\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    total_loss = 0\n","\n","    for i, data in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        x, y, lx, ly = data\n","        x, y = x.to(device), y.to(device)\n","\n","        with torch.cuda.amp.autocast():\n","            h, lh = model(x, lx)\n","            h = torch.permute(h, (1, 0, 2))\n","            loss = criterion(h, y, lh, ly)\n","\n","        total_loss += loss.item()\n","\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","        # Another couple things you need for FP16.\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update() # This is something added just for FP16\n","\n","        del x, y, lx, ly, h, lh, loss\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    return total_loss / len(train_loader)\n","\n","\n","def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","\n","    total_loss = 0\n","    vdist = 0\n","\n","    for i, data in enumerate(val_loader):\n","\n","        x, y, lx, ly = data\n","        x, y = x.to(device), y.to(device)\n","\n","        with torch.inference_mode():\n","            h, lh = model(x, lx)\n","            h = torch.permute(h, (1, 0, 2))\n","            loss = criterion(h, y, lh, ly)\n","\n","        total_loss += float(loss)\n","        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n","\n","        batch_bar.update()\n","\n","        del x, y, lx, ly, h, lh, loss\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    total_loss = total_loss/len(val_loader)\n","    val_dist = vdist/len(val_loader)\n","    return total_loss, val_dist"]},{"cell_type":"markdown","metadata":{"id":"qpYExu4vT4_g"},"source":["## Training Setup"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"husa5_EYMUz6"},"outputs":[],"source":["def save_model(model, optimizer, scheduler, metric, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         'scheduler_state_dict'     : scheduler.state_dict(),\n","         metric[0]                  : metric[1],\n","         'epoch'                    : epoch},\n","         path\n","    )\n","\n","def load_model(path, model, metric= 'valid_dist', optimizer= None, scheduler= None):\n","\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    if optimizer != None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    if scheduler != None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","    epoch   = checkpoint['epoch']\n","    metric  = checkpoint[metric]\n","\n","    return [model, optimizer, scheduler, epoch, metric]"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"tExvyl1BIdMC"},"outputs":[],"source":["# This is for checkpointing, if you're doing it over multiple sessions\n","\n","\n","last_epoch_completed = 0\n","start = last_epoch_completed\n","end = config[\"epochs\"]\n","best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n","epoch_model_path =  '/home/22941/epoch_model_path.pth'#set the model path( Optional, you can just store best one. Make sure to make the changes below )\n","best_model_path = '/home/22941/best_model_path.pth'#TODO set best model path"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model: ASRModel(\n","  (augmentations): Sequential()\n","  (encoder): Encoder(\n","    (PermuteBlock): PermuteBlock()\n","    (embedding): Sequential(\n","      (0): Conv1d(27, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): GELU(approximate='none')\n","      (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): GELU(approximate='none')\n","      (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (8): GELU(approximate='none')\n","    )\n","    (lstm_layers): ModuleList(\n","      (0): pBLSTM(\n","        (blstm): LSTM(512, 384, num_layers=3, dropout=0.3, bidirectional=True)\n","      )\n","      (1): LockedDropout(p=0.25)\n","      (2): pBLSTM(\n","        (blstm): LSTM(1536, 384, num_layers=3, dropout=0.3, bidirectional=True)\n","      )\n","      (3): LockedDropout(p=0.25)\n","    )\n","  )\n","  (decoder): Decoder(\n","    (mlp): Sequential(\n","      (0): PermuteBlock()\n","      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): PermuteBlock()\n","      (3): Linear(in_features=768, out_features=128, bias=True)\n","      (4): GELU(approximate='none')\n","      (5): Linear(in_features=128, out_features=64, bias=True)\n","      (6): GELU(approximate='none')\n","      (7): Linear(in_features=64, out_features=41, bias=True)\n","    )\n","    (softmax): LogSoftmax(dim=2)\n","  )\n",")\n"," optimizer: AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    lr: 0.0008192000000000002\n","    maximize: False\n","    weight_decay: 0.01\n",")\n"," scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f389c37fa58>\n"," epoch: 35\n"," metric: 4.668895348837209\n","\n"]}],"source":["model,optimizer,scheduler,epochs,metric= load_model(path=best_model_path,model=model,optimizer=optimizer,scheduler=scheduler)\n","# manually change the optimizer state\n","# optimizer.param_groups[0]['lr'] = 2e-4\n","# \n","print(f'model: {model}\\n',f'optimizer: {optimizer}\\n',f'scheduler: {scheduler}\\n',f'epoch: {epochs}\\n',f'metric: {metric}\\n')\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"JR43E28rM9Ak"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch: 1/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 3.4057\t Learning Rate 0.0020000\n","\tVal Dist 69.4148%\t Val Loss 3.3000\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 2/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 2.4861\t Learning Rate 0.0020000\n","\tVal Dist 27.0820%\t Val Loss 1.2052\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 3/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 1.0008\t Learning Rate 0.0020000\n","\tVal Dist 15.9071%\t Val Loss 0.7239\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 4/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.7192\t Learning Rate 0.0020000\n","\tVal Dist 12.2523%\t Val Loss 0.5619\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 5/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.6041\t Learning Rate 0.0020000\n","\tVal Dist 10.4472%\t Val Loss 0.4801\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 6/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.5286\t Learning Rate 0.0020000\n","\tVal Dist 9.2939%\t Val Loss 0.4338\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 7/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.4774\t Learning Rate 0.0020000\n","\tVal Dist 8.8095%\t Val Loss 0.4144\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 8/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.4378\t Learning Rate 0.0020000\n","\tVal Dist 7.9649%\t Val Loss 0.3766\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 9/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.4155\t Learning Rate 0.0020000\n","\tVal Dist 7.7082%\t Val Loss 0.3714\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 10/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3852\t Learning Rate 0.0020000\n","\tVal Dist 7.2935%\t Val Loss 0.3475\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 11/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3705\t Learning Rate 0.0020000\n","\tVal Dist 7.1881%\t Val Loss 0.3443\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 12/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3510\t Learning Rate 0.0020000\n","\tVal Dist 6.8393%\t Val Loss 0.3312\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 13/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3353\t Learning Rate 0.0020000\n","\tVal Dist 6.7756%\t Val Loss 0.3356\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 14/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3266\t Learning Rate 0.0020000\n","\tVal Dist 6.6070%\t Val Loss 0.3314\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 15/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.3158\t Learning Rate 0.0020000\n","\tVal Dist 6.3494%\t Val Loss 0.3202\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 16/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2971\t Learning Rate 0.0020000\n","\tVal Dist 6.3296%\t Val Loss 0.3209\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 17/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2914\t Learning Rate 0.0020000\n","\tVal Dist 6.1091%\t Val Loss 0.3103\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 18/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2819\t Learning Rate 0.0020000\n","\tVal Dist 6.1229%\t Val Loss 0.3083\n","Saved epoch model\n","\n","Epoch: 19/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2752\t Learning Rate 0.0020000\n","\tVal Dist 5.9623%\t Val Loss 0.3061\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 20/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2653\t Learning Rate 0.0020000\n","\tVal Dist 5.8691%\t Val Loss 0.3025\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 21/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2716\t Learning Rate 0.0020000\n","\tVal Dist 5.7870%\t Val Loss 0.2971\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 22/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2563\t Learning Rate 0.0020000\n","\tVal Dist 5.7264%\t Val Loss 0.2937\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 23/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2508\t Learning Rate 0.0020000\n","\tVal Dist 5.5070%\t Val Loss 0.2944\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 24/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2437\t Learning Rate 0.0020000\n","\tVal Dist 5.8229%\t Val Loss 0.3054\n","Saved epoch model\n","\n","Epoch: 25/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2463\t Learning Rate 0.0020000\n","\tVal Dist 5.6554%\t Val Loss 0.2957\n","Saved epoch model\n","\n","Epoch: 26/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2227\t Learning Rate 0.0016000\n","\tVal Dist 5.3176%\t Val Loss 0.2821\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 27/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2144\t Learning Rate 0.0016000\n","\tVal Dist 5.1831%\t Val Loss 0.2782\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 28/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2073\t Learning Rate 0.0016000\n","\tVal Dist 5.2796%\t Val Loss 0.2886\n","Saved epoch model\n","\n","Epoch: 29/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.2089\t Learning Rate 0.0016000\n","\tVal Dist 5.1708%\t Val Loss 0.2845\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 30/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1926\t Learning Rate 0.0012800\n","\tVal Dist 5.0038%\t Val Loss 0.2772\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 31/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1920\t Learning Rate 0.0012800\n","\tVal Dist 4.9870%\t Val Loss 0.2775\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 32/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1844\t Learning Rate 0.0012800\n","\tVal Dist 4.9770%\t Val Loss 0.2766\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 33/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1750\t Learning Rate 0.0010240\n","\tVal Dist 4.7753%\t Val Loss 0.2762\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 34/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1693\t Learning Rate 0.0010240\n","\tVal Dist 4.7401%\t Val Loss 0.2728\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 35/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1688\t Learning Rate 0.0010240\n","\tVal Dist 4.8311%\t Val Loss 0.2773\n","Saved epoch model\n","\n","Epoch: 36/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1591\t Learning Rate 0.0008192\n","\tVal Dist 4.6689%\t Val Loss 0.2752\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 37/50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.1537\t Learning Rate 0.0008192\n","\tVal Dist 4.7284%\t Val Loss 0.2806\n","Saved epoch model\n","\n","Epoch: 38/50\n"]},{"name":"stderr","output_type":"stream","text":["Train:   3%|▎         | 15/446 [00:11<05:29,  1.31it/s, loss=0.1496, lr=0.000819]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1334/3049359526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dist\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphoneme_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1334/4244977070.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Another couple things you need for FP16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is something added just for FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","#TODO: Please complete the training loop\n","\n","for epoch in range(0, 100):\n","\n","    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n","\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","\n","    train_loss              = train_model(model=model,train_loader=train_loader,criterion=criterion,optimizer=optimizer)\n","    valid_loss, valid_dist  = validate_model(model=model,val_loader=val_loader,decoder=decoder,phoneme_map=LABELS)\n","    scheduler.step(valid_dist)\n","\n","    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n","    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n","\n","\n","    wandb.log({\n","        'train_loss': train_loss,\n","        'valid_dist': valid_dist,\n","        'valid_loss': valid_loss,\n","        'lr'        : curr_lr\n","    })\n","\n","    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n","    wandb.save(epoch_model_path)\n","    print(\"Saved epoch model\")\n","\n","    if valid_dist <= best_lev_dist:\n","        best_lev_dist = valid_dist\n","        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n","        wandb.save(best_model_path)\n","        print(\"Saved best model\")\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"M2H4EEj-sD32"},"source":["# Generate Predictions and Submit to Kaggle"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"2moYJhTWsOG-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["64\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 41/41 [00:44<00:00,  1.10s/it]"]},{"name":"stdout","output_type":"stream","text":["60\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#TODO: Make predictions\n","\n","# Follow the steps below:\n","# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n","# 2. Get prediction string by decoding the results of the beam decoder\n","\n","TEST_BEAM_WIDTH = 10\n","\n","test_decoder  = CTCBeamDecoder(\n","    labels = LABELS, #\n","    model_path=None,\n","    alpha=0,\n","    beta=0,\n","    cutoff_top_n=40,\n","    cutoff_prob=1.0,\n","    beam_width=TEST_BEAM_WIDTH,\n","    num_processes=1,\n","    blank_id=0,\n","    log_probs_input=True\n",")\n","results = []\n","\n","model.eval()\n","print(\"Testing\")\n","for data in tqdm(test_loader):\n","\n","    x, lx   = data\n","    x       = x.to(device)\n","\n","    with torch.no_grad():\n","        h, lh = model(x, lx)\n","\n","    prediction_string= decode_prediction(output=h,output_lens=lh,decoder=test_decoder,PHONEME_MAP=LABELS) # TODO call decode_prediction\n","    print(len(prediction_string))\n","    #TODO save the output in results array.\n","    for string in prediction_string:\n","        results.append(string)\n","    del x, lx, h, lh\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"d70dvu_lsMlv"},"outputs":[],"source":["data_dir = f\"11-785-s24-hw3p2//test-clean/random_submission.csv\"\n","df = pd.read_csv(data_dir)\n","df.label = results\n","df.to_csv('submission.csv', index = False)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"m1sZmEIs4yIz"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (5.2.0) doesn't match a supported version!\n","  RequestsDependencyWarning)\n","Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/22941/.kaggle/kaggle.json'\n","Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.7 / client 1.5.8)\n","100%|█████████████████████████████████████████| 209k/209k [00:00<00:00, 332kB/s]\n","Successfully submitted to HW3P2_ASR-S24"]}],"source":["!kaggle competitions submit -c hw3p2asr-s24 -f submission.csv -m \"I made it!\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["HLad4pChcuvX","rd5aNaLVoR_g","qpYExu4vT4_g","M2H4EEj-sD32"],"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"vscode":{"interpreter":{"hash":"bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"}}},"nbformat":4,"nbformat_minor":0}
